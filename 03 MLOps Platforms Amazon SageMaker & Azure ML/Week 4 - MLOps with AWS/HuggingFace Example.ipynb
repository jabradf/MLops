{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deploy A Hugging Face Pretrained Model to Amazon SageMaker Serverless Endpoint - Boto3\n",
    "Amazon SageMaker Studio Lab provides you a free environment to develop an experimental project, which is suitable for ML learners to start their first ML lesson. The CPU session is avaliable for continously 12 hours, and GPU session is avaliable for continously 4 hours.\n",
    "\n",
    "What if you want to have a think big project for your ML learning, for instances, you want to make the model public avaliable instead of running the model locally, what can be your options? Are you thinking about the same thing? Deploy to an Amazon SageMaker Endpoint!\n",
    "\n",
    "In this tutorial, I will share you how to deploy a Hugging Face pre-trained model to an Amazon SageMaker Serverless Endpoint. By showing you:\n",
    "\n",
    "1. Introduce the model\n",
    "2. Set up the pre-requisition - AWS user, Amazon SageMaker execution role\n",
    "3. Connect your AWS resources(SageMaker) using boto3\n",
    "4. Create a SageMaker model with boto3 SageMaker Client\n",
    "5. Create a serverless endpoint configuration\n",
    "6. Test with boto3 SageMaker run time to invoke the endpoint\n",
    "7. Delete the model, the endpoint config, and the endpoint\n",
    "\n",
    "![SageMaker Deployment](02-serverless-endpoint.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1. Introduce the model\n",
    "Hugging Face is a community and data science platform. Which provides tools that enable users to build, train and deploy ML models based on open source code and technologies. In their website, you may see open source model and dataset in their [website](https://huggingface.co/).\n",
    "\n",
    "`distilbert-base-uncased-finetuned-sst-2-english` model is an English text classification model based on the DistilBERT. [DistilBERT](https://huggingface.co/distilbert-base-uncased) is a transformers model, which is smaller and faster than BERT. It was pretrained on the same corpus and used the BERT base model as a teacher.\n",
    "\n",
    "You can test the model with python code as bellow. You can also test the model in the Hosted inference API session on the page: https://huggingface.co/distilbert-base-uncased-finetuned-sst-2-english"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install huggingface_hub==0.1.0 \n",
    "%pip install transformers==4.12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "from transformers import TextClassificationPipeline\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-uncased-finetuned-sst-2-english\")\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\"distilbert-base-uncased-finetuned-sst-2-english\")\n",
    "\n",
    "pipe = TextClassificationPipeline(model=model, tokenizer=tokenizer, return_all_scores=True)\n",
    "pipe(\"I love Amazon SageMaker Studio Lab!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2. Set up the pre-requisition\n",
    "### 2.1 AWS user login\n",
    "You will need to install AWS CLI, boto3, and configure with your AWS credentials. You can find the credentials from the IAM > users.\n",
    "\n",
    "If you don't have users, create a new one. If you already have users, select one, and get the credentials in the Security credentials tab.\n",
    "* Boto3\n",
    "\n",
    "Boto3 is the AWS SDK for Python (Boto3). Which you can use it to create, configure, and manage AWS services. The SDK provides an object-oriented API as well as low-level access to AWS services. To learn more about boto3: https://boto3.amazonaws.com/v1/documentation/api/latest/guide/quickstart.html\n",
    "\n",
    "AWS CLI\n",
    "The AWS Command Line Interface (AWS CLI) is a unified tool to manage your AWS services. With just one tool to download and configure, you can control multiple AWS services from the command line and automate them through scripts. To learn more about AWS CLI: https://aws.amazon.com/cli/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install boto3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install awscli"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile ~/.aws/credentials\n",
    "\n",
    "[default]\n",
    "aws_access_key_id =  < paste your access key here, run this cell, then delete the cell >\n",
    "aws_secret_access_key = < paste your secret key here, run this cell, then delete the cell > "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile ~/.aws/config\n",
    "\n",
    "[default]\n",
    "region=us-east-1 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if you don't want to setup the key/secret with credentials, you can setup the environment variable with\n",
    "ACCESS_KEY= #< paste your access key here, run this cell, then delete the cell >\n",
    "SECRET_KEY= #< paste your secret key here, run this cell, then delete the cell > "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 Amazon SageMaker execution role\n",
    "Amazon SageMaker execution role is an AWS Identity and Access Management (IAM) role that performs operations on your behalf on the AWS hardware that is managed by SageMaker. A SageMaker user can grant these permissions with an IAM role (referred to as an execution role).\n",
    "\n",
    "You can create an execution role by following the [instruction](https://docs.aws.amazon.com/sagemaker/latest/dg/sagemaker-roles.html). If you are already used to using SageMaker within your own AWS account, please copy and paste the arn for your execution role below.\n",
    "\n",
    "Please note, in order to complete this you will need to have already created this SageMaker IAM Execution role."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sagemaker\n",
    "\n",
    "# create a sagemaker execution role via the AWS SageMaker console, then paste in the arn here\n",
    "role = ' < paste your execution role here > '"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step3. connect your AWS resources(SageMaker) using boto3\n",
    "Amazon SageMaker Serverless Inference is a purpose-built inference option that makes it easy for you to deploy and scale ML models. Serverless Inference is ideal for workloads which have idle periods between traffic spurts and can tolerate cold starts. Serverless endpoints automatically launch compute resources and scale them in and out depending on traffic, eliminating the need to choose instance types or manage scaling policies. This takes away the undifferentiated heavy lifting of selecting and managing servers. Serverless Inference integrates with AWS Lambda to offer you high availability, built-in fault tolerance and automatic scaling. Here is the pseudo code to show you the process to create a Amazon SageMaker serverless endpoint.\n",
    "```\n",
    "container = {\n",
    "    'Image': CONTAINER_URL, \n",
    "    'ModelDataUrl': ‘s3://my-bucket/path/to/artifacts/’, #applies to bring your own model case\n",
    "    'Mode': 'SingleModel' } \n",
    "\n",
    "response = sm_client.create_model( \n",
    "    ModelName = 'model-name',\n",
    "    PrimaryContainer=container_config,\n",
    "    ExecutionRoleArn=role, \n",
    "    EnableNetworkIsolation=False) \n",
    "\n",
    "response = sm_client.create_endpoint_config(    \n",
    "     EndpointConfigName = ‘my-epc’, \n",
    "     ServerlessConfig=[{…,…}])\n",
    "\n",
    "response = sm_client.create_endpoint(EndpointName = ‘my-endpoint’,\n",
    "                                     EndpointConfigName = ‘my-epc’)\n",
    "```\n",
    "## How Amazon SageMaker Serverless Endpoints Works:\n",
    "![Serverless Endpoints](03-serverless-endpoints-how-it-works.png)\n",
    "\n",
    "\n",
    "more reference for serverless\n",
    "* https://towardsdatascience.com/sagemaker-serverless-inference-is-now-generally-available-e42550a146fe\n",
    "* https://docs.aws.amazon.com/sagemaker/latest/dg/serverless-endpoints.html\n",
    "* https://aws.amazon.com/about-aws/whats-new/2021/12/amazon-sagemaker-serverless-inference/\n",
    "* https://github.com/huggingface/notebooks/blob/main/sagemaker/11_deploy_model_from_hf_hub/deploy_transformer_model_from_hf_hub.ipynb\n",
    "* [Host Hugging Face transformer models using Amazon SageMaker Serverless Inference](https://aws.amazon.com/blogs/machine-learning/host-hugging-face-transformer-models-using-amazon-sagemaker-serverless-inference/)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "sm_client = boto3.client('sagemaker',\n",
    "                        aws_access_key_id=ACCESS_KEY,\n",
    "                        aws_secret_access_key=SECRET_KEY)\n",
    "response = sm_client.list_endpoints()\n",
    "\n",
    "len(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "ml_model_name = \"text-classification-hugging-face\"\n",
    "timestamp = time.strftime('-%Y-%m-%d-%H-%M-%S', time.gmtime())\n",
    "model_name = ml_model_name + '-model' + timestamp\n",
    "endpoint_config_name = ml_model_name + '-epc' + timestamp\n",
    "print(model_name)\n",
    "print(endpoint_config_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4. create a SageMaker model with boto3 SageMaker Client\n",
    "* Before create an endpoint, you need to create a model to deploy first. Here shows how to use SageMaker Boto3 client to create a model. The document can be found here: [boto3 sagemaker client create model](https://boto3.amazonaws.com/v1/documentation/api/latest/reference/services/sagemaker.html#SageMaker.Client.create_model)\n",
    "* For each model, you will need to specify which Amazon SageMaker container you are going to use in this Amazon SageMaker endpoint. AWS has prebuilt deep learning containers for 5 software frameworks, including TensorFlow, PyTorch, MXNet, Hugging Face, and AutoGloun. We call it: __Deep Learning Containers(DLC)__. It depends on the use case, which can be a Tensorflow based, Pytorch based DLC image. In this example, we use Hugging Face with Pytorch as the container image. All the avalible open source framework is documented [here](https://github.com/aws/deep-learning-containers/blob/master/available_images.md). The URL composed with two part, `repository URL by region + framework version with job type, hardware spec, and python version`.\n",
    "* To learn more about script mode on SageMaker, check out our documentation [here](https://sagemaker.readthedocs.io/en/stable/frameworks/index.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_data_url=\"763104351884.dkr.ecr.us-east-1.amazonaws.com/huggingface-pytorch-inference:1.9-transformers4.12-cpu-py38-ubuntu20.04\"\n",
    "\n",
    "container_config = {'Image': model_data_url,\n",
    "                    'Mode': 'SingleModel',\n",
    "                    'Environment': {\n",
    "                        'HF_MODEL_ID': 'distilbert-base-uncased-finetuned-sst-2-english',\n",
    "                        'HF_TASK' : 'text-classification',\n",
    "                        'SAGEMAKER_CONTAINER_LOG_LEVEL' : '20',\n",
    "                        'SAGEMAKER_REGION' : 'us-east-1'\n",
    "                    }\n",
    "                   }\n",
    "\n",
    "response = sm_client.create_model(\n",
    "    ModelName=model_name,\n",
    "    PrimaryContainer=container_config,\n",
    "    ExecutionRoleArn=role, \n",
    "    EnableNetworkIsolation=False\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Check model is created by visiting: Console > Amazon SageMaker > Models\n",
    "## step 5. create a serverless endpoint configuration\n",
    "* In this step, we will create an endpoint configuration with ServerlessConfig, which can be used in __sm_client.create_endpoint__. The configuration detail is listed [here](https://boto3.amazonaws.com/v1/documentation/api/latest/reference/services/sagemaker.html#SageMaker.Client.create_endpoint_config).\n",
    "* The endpoint is not created until you run the __sm_client.create_endpoint__. After running this step, you will need to wait about 2 minutes for the model deployment. The detail of create endpoint API is available [here](https://boto3.amazonaws.com/v1/documentation/api/latest/reference/services/sagemaker.html#SageMaker.Client.create_endpoint)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "endpoint_config_response = sm_client.create_endpoint_config(\n",
    "   EndpointConfigName=endpoint_config_name,\n",
    "   ProductionVariants=[\n",
    "        {\n",
    "            \"ModelName\": model_name,\n",
    "            \"VariantName\": \"AllTraffic\",\n",
    "            \"ServerlessConfig\": {\n",
    "                # Specify MemorySizeInMB and MaxConcurrency in the serverless config object\n",
    "                \"MemorySizeInMB\": 4096,\n",
    "                \"MaxConcurrency\": 10\n",
    "            }\n",
    "        }\n",
    "    ]\n",
    ")\n",
    "\n",
    "print('Endpoint configuration name: {}'.format(endpoint_config_name))\n",
    "print('Endpoint configuration arn:  {}'.format(endpoint_config_response['EndpointConfigArn']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "endpoint_name = \"studio-lab-ep\" + '-epc' + timestamp\n",
    "response = sm_client.create_endpoint(\n",
    "    EndpointName=endpoint_name,\n",
    "    EndpointConfigName=endpoint_config_name\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* check endpoint is created!\n",
    "  * Console > Amazon SageMaker > Endpoints\n",
    "\n",
    "## step 6. test with boto3 SageMaker run time to invoke the endpoint\n",
    "After checking the endpoint is ready, we now use SageMaker Runtime to interact with the endpoint. According to the [documentation](https://docs.aws.amazon.com/sagemaker/latest/dg/serverless-endpoints-invoke.html), you will need:\n",
    "\n",
    "(1) endpoint_name, use the name of the in-service serverless endpoint you want to invoke.\n",
    "(2) content_type, specify the MIME type of your input data in the request body (for example, application/json). If you want to learn more about what are the other options for content type, you can visit [Common Data Formats for Inference](https://docs.aws.amazon.com/sagemaker/latest/dg/cdf-inference.html) to understand more.\n",
    "(3) payload, use your request payload for inference. Your payload should be in bytes or a file-like object.\n",
    "Now, let's try out the endpoint!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import boto3\n",
    "runtime = boto3.client(\"sagemaker-runtime\",\n",
    "                       aws_access_key_id=ACCESS_KEY,\n",
    "                       aws_secret_access_key=SECRET_KEY) \n",
    "\n",
    "content_type = \"application/json\"\n",
    "\n",
    "# example request, you always need to define \"inputs\"\n",
    "data = {\n",
    "   \"inputs\": \"Happy Birthday to you!\"\n",
    "}\n",
    "\n",
    "response = runtime.invoke_endpoint(\n",
    "    EndpointName=endpoint_name,\n",
    "    ContentType=content_type,\n",
    "    Body=json.dumps(data)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response[\"Body\"].read().decode()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## step 7. delete the model, the endpoint config, and the endpoint\n",
    "Now, if we don't need the model, or the endpoint anymore. We will need to clean up the artifacts that we created earlier. According to the documentation:\n",
    "\n",
    "_You must not delete an EndpointConfig that is in use by an endpoint that is live or while the UpdateEndpoint or CreateEndpoint operations are being performed on the endpoint. To update an endpoint, you must create a new EndpointConfig ._\n",
    "Let's delete the artifacts in order."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sm_client.delete_endpoint(EndpointName=endpoint_name)\n",
    "sm_client.delete_endpoint_config(EndpointConfigName=endpoint_config_name)\n",
    "sm_client.delete_model(ModelName=model_name)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
